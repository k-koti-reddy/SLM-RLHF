{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvXAQszyGySa"
      },
      "source": [
        "# Small Language Model with Reinforcement Learning.\n",
        "- The goal of this project is to demonstrate the importance of reinceforcement learning and its performance on small language model.\n",
        "\n",
        "## Important tasks:\n",
        "- Pretrain Small Language Model (10-100 million parameters)\n",
        "- RLHF pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HatifyS8GySf"
      },
      "source": [
        "### What is small language model (SLM)?\n",
        "- The difference between small language model and large language model is in its training dataset, the transformer architecture remains same. It sole purpose is to pretrain language model for specific task whereas LLM can do multiple tasks. Thereby a good quality dataset is important for our SLM to capture the grammer and context of a sentence and the task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D8_yvn1GySg"
      },
      "source": [
        "### Dataset\n",
        "- For the simplicity, we load the dataset of tiny stories mentioned in this paper https://arxiv.org/abs/2305.07759, we can find the dataset from hugging face."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1RdenDvGySh"
      },
      "source": [
        "### Step 1: Import Dataset\n",
        "TinyStories is a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We can get it from HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuZIC38ZGySh",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qBv7-mMEGySj",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VQRxg1IGySk",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"roneneldan/TinyStories\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM6O7JYhJKTA"
      },
      "source": [
        "- Peek into the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSuYflbTG7oz"
      },
      "outputs": [],
      "source": [
        "print('Total Number of training data', len(ds['train']))\n",
        "print('Total Number of validation data', len(ds['validation']))\n",
        "print(\"\\n============Training Sample==============\\n\")\n",
        "print(ds['train'][0])\n",
        "print(\"\\n============Validation Sample============\\n\")\n",
        "print(ds['validation'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenize Dataset\n",
        "- In this section tokenize the dataset into tokenIDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Beu06z4uH918"
      },
      "outputs": [],
      "source": [
        "!pip install tiktoken\n",
        "import tiktoken\n",
        "\n",
        "encoder = tiktoken.get_encoding(\"gpt-2\")\n",
        "\n",
        "# process function takes a sample from the dataset and returns ids and length of ids\n",
        "def process(sample):\n",
        "    text = sample['text']\n",
        "    ids = encoder.encode_ordinary(text) # encode_ordinary to avoid adding special tokens\n",
        "    return {'ids': ids, 'len': len(ids)}\n",
        "\n",
        "tokenized = ds.map(process, remove_columns=['text'], num_proc=4)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
