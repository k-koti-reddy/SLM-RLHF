{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvXAQszyGySa"
      },
      "source": [
        "# Small Language Model with Reinforcement Learning.\n",
        "- The goal of this project is to demonstrate the importance of reinceforcement learning and its performance on small language model.\n",
        "\n",
        "## Important tasks:\n",
        "- Pretrain Small Language Model (10-100 million parameters)\n",
        "- RLHF pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HatifyS8GySf"
      },
      "source": [
        "### What is small language model (SLM)?\n",
        "- The difference between small language model and large language model is in its training dataset, the transformer architecture remains same. It sole purpose is to pretrain language model for specific task whereas LLM can do multiple tasks. Thereby a good quality dataset is important for our SLM to capture the grammer and context of a sentence and the task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D8_yvn1GySg"
      },
      "source": [
        "### Dataset\n",
        "- For the simplicity, we load the dataset of tiny stories mentioned in this paper https://arxiv.org/abs/2305.07759, we can find the dataset from hugging face."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1RdenDvGySh"
      },
      "source": [
        "### Step 1: Import Dataset\n",
        "TinyStories is a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We can get it from HuggingFace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuZIC38ZGySh",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qBv7-mMEGySj",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VQRxg1IGySk",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"roneneldan/TinyStories\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM6O7JYhJKTA"
      },
      "source": [
        "- Peek into the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSuYflbTG7oz"
      },
      "outputs": [],
      "source": [
        "print('Total Number of training data', len(ds['train']))\n",
        "print('Total Number of validation data', len(ds['validation']))\n",
        "print(\"\\n============Training Sample==============\\n\")\n",
        "print(ds['train'][0])\n",
        "print(\"\\n============Validation Sample============\\n\")\n",
        "print(ds['validation'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bee1044a"
      },
      "source": [
        "## Next Steps:\n",
        "\n",
        "1.  **Tokenization**: Convert the text data into numerical tokens that the language model can understand.\n",
        "2.  **Data Preparation**: Create data loaders or tensors for training and evaluation.\n",
        "3.  **Model Definition**: Define the architecture of your small language model.\n",
        "4.  **Training Setup**: Set up the training loop, including optimizer, loss function, and training parameters.\n",
        "5.  **Training**: Train the model on the prepared dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenization\n",
        "- convert text to tokenIDs\n",
        "- write all token ids to the .bin file in batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install tiktoken\n",
        "import tiktoken\n",
        "import os\n",
        "import numpy as np\n",
        "import tqdm.auto as tqdm\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "def process_sample(sample):\n",
        "    text = sample[\"text\"]\n",
        "    tokens = tokenizer.encode_ordinary(text) # Use encode_ordinary to avoid adding special tokens\n",
        "    return {'ids': tokens, 'len': len(tokens)}\n",
        "\n",
        "# write all tokens to the memory mapped file\n",
        "if not os.path.exists('train.bin'):\n",
        "    # Generate tokenized dataset\n",
        "    tokenized_ds = ds.map(process_sample, remove_columns=['text'], num_proc=8)\n",
        "    \n",
        "    for split, dataset in tokenized_ds.items():\n",
        "        \n",
        "        token_len = np.sum(dataset['len'], dtype=np.uint64)\n",
        "        print(f'Total token length in {split} data: {token_len}')\n",
        "        \n",
        "        arr = np.memmap(f\"{split}.bin\", dtype=np.uint16, mode='w+', shape=(token_len,))\n",
        "        total_batches = 1024\n",
        "        train_idx = 0\n",
        "        for batch_id in tqdm(range(total_batches), desc='Writing to train.bin'):\n",
        "            # write the tokenids to mem map in batches\n",
        "            batch = dataset.shard(num_shards=total_batches, index=batch_id, contiguous=True).with_format('numpy')\n",
        "            batch_ids = np.concatenate(batch['ids'])\n",
        "            arr[train_idx : train_idx+len(batch_ids)] = batch_ids\n",
        "            train_idx += len(batch_ids)\n",
        "        arr.flush()\n",
        "        print('train data written to train.bin')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
